% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{3cm}
%
% and set <dim> to something 5cm or larger.

\title{Measuring Music Quality Using JETSING \\ (JET Serialism Is Not Good) }

\author{Jonathan Sabini \And Ethan Fleming \And Taylor Noah 
\AND
Portland State University, 2022}

\begin{document}
\maketitle

\begin{abstract}
    The most advanced music generation models to date unsurprisingly use musical theory to work effectively.
    However, the metrics used to score these music generation models are intended for generic machine learning purposes.
    Often times this leads to many models sounding dramatically different, while having scores that are indistinguishable.
    Additionally these metrics are dependent on a model's internal weights, rather than scoring the model for its music alone.
    Shouldn't the metric scoring the models also be designed with music theory in mind?
\end{abstract}

\section{Introduction}
Music is inherently repetitive, reusing motifs to form a compelling melody that's self-coherent throughout the piece.
Many current models for music generation often have no notion of a long-term structure in musical composition and sound,
as though the computer musician is wandering around unsure how to compose a consistent rhythm.
The Music Transformer model \citep{huang2018music} uses close attention to overcome this issue,
maintaining long-range coherence throughout its generated musical pieces.

Although the music sounds drastically better than models that don't use relative self-attention,
the negative log-likelihood metric's numerical results don't demonstrate this difference well.
Many metrics, including the negative log-likelihood, can only be utilized on a machine learning model and wouldn't be able to judge a stand-alone music piece.
In general, from what we have found, most metrics used to evaluate music generation models have little foundation on musical theory.

\section{Related Work}
Countless papers have been written on the topic of music generation, but many of them only use an understanding of musical theory to improve the models themselves.
\citep{huang2018music} worked with Vaswani to utilize the self-attention mechanisms of their original transformer model from \citep{vaswani2017attention}, but adjusting it for music generation.
\citep{kotecha2018generating} used a bi-axial LSTM to generate polyphonic music aligned with musical rules.
Finally, \citep{zhao2020verticalhorizontal} using an extensive knowledge of musical theory worked to create a lightweight variational auto-encoder model.
While many papers have used musical theory extensively to improve music generation models, we hope to create a metric that will simplify the process of assessing these models numerically.

\section{Motivation}
Our group has an interest and background in music, so we wanted to incorporate this knowledge into our machine learning research.
There are many models for music generation, but many popularly used metrics to judge them result in dramatically different qualities of musical pieces being considered relatively the same.
Many music papers in artificial intelligence, to our knowledge, are often forced to resort to qualitatively comparing generated pieces when numerical metrics fail to show the contrast.

We want a metric that will more effectively ascertain how tasteful a composition sounds or perhaps how distasteful.
Listening to the sample music generated by the relative self-attention transformer model inspired us to do something on this topic.

\subsection{Definition of Serialism}

We propose a metric based on the rules of serialism in western music theory, which we call the $JETSING$ metric.
In western music, which most of us are accustomed to, there is a central tonality from which notes and keys are chosen to sound “good.”
Conversely, serialism has no central tonality. Instead, it is based on the concept of a “row.”
A row is a random ordering of the 12 unique half-step pitches in western music theory (C - B on the piano).
In strict serialism, you may not repeat a pitch until all 12 pitches in a row have been played, the row must always appear in the same order, and notes are allowed to be in any octave.
Because there is no central tonality in this method, music based on serialism tends to sound creepy, confusing, lost, and seemingly aimless.

Many models that audibly perform worse tend to have characteristics of serialism.
They wander aimlessly, do not repeat motifs, and lack a sense of central tonality.
Conversely, the models that perform well tend to have central tonality and can repeat motifs and create a long-term sense of direction in the piece.

The rules of serialism\footnote{\citep{whittall2008serialism}} are as follows:
\begin{enumerate}
    \item No note should be repeated until all 12 notes of a note row have been played.
    \item The order of the row remains consistent throughout the piece.
    \item Notes can be played at any octave.
\end{enumerate}

\subsection{Main Goal}

Our $JETSING$ metric is based on the rules of serialism and captures how well a piece performs with regard to those rules.
In other words, we measure how serial a piece is.
The less serial a piece is, the better it adheres to western music theory and likely sounds “good” to our ears.
$JETSING$ is comprised of two sub metrics $JETSING_{ROW}$ and $JETSING_{OCT}$.
The former metric measures how well a piece adheres to the first two rules and the latter metric measures the average octave difference between each pair of melodic notes.

\section{Method}
\subsection{Parsing MIDI Files}

The goal of the MIDI parser is to obtain only the melody and disregard the rest.
The parser is set up to process any MIDI file into a format with which our metric can easily make calculations.
The parser uses the Python library music21 to output an object containing notes from the MIDI input.
The object is a list of notes or chords depending on whether one note or several notes are played at the same time.

The parser is designed to first try and separate the melody and bass by detecting a midpoint in the music.
The parser follows the C scale notation where the lowest note is C and the highest is B.

\begin{table}
    \centering
    \begin{tabular}{c c}
        \hline
        Note & Value \\
        \hline
        C & 3 \\
        \hline
        D & 4 \\
        \hline
        E & 5 \\
        \hline
        F & 6 \\
        \hline
        G & 7 \\
        \hline
        A & 8 \\
        \hline
        B & 9 \\
        \hline
    \end{tabular}
    \caption{The parser assigns values 3 - 9 to C - B.}
\end{table}

These values are those used in calculating the mean of the notes in a MIDI file.
As for octaves, the parser is using the octave information given by the music21 MIDI decoder. The octave range is from 1 to 10.
The parser will total up the values for each note and octave separately and divide them with the total notes in the MIDI to get the middle note.

\begin{itemize}
    \item $ \mu_{note} = \frac{\sum note}{cardinality_{note}(MIDI)} $ 
    \item $ \mu_{octave} = \frac{\sum octave}{cardinality_{octave}(MIDI)} $
    \item middle note $= \mu_{note} || \mu_{octave} $
\end{itemize}

With the middle note obtained, the parser now roughly knows whether a note is part of the bass or melody.
If a note is higher than the middle note, then it is considered part of the melody.
Otherwise, the parser considers it as part of the bass.

\subsection{$JETSING$}
Based on the rules of serialism, we first create two sub-metrics of $JETSING$: $JETSING_{ROW}$ and $JETSING_{OCT}$.

\subsubsection{$JETSING_{ROW}$}
$JETSING_{ROW}$ will be responsible for the first two rules.
It begins at 0 and increments for each break of rules 1 and 2. 
We divide this sum by the highest possible number of correct tone rows the piece could have had.

Here are examples of valid and invalid note orderings using just 5 tones:
\begin{itemize}
    \item Valid Row: A-B-C-D-E 
    \item Invalid (Violates 1): A-A-B-C-D-E
    \begin{itemize}
        \item All notes are accounted for, but a note is repeated before all notes are played.
    \end{itemize}
    \item Invalid (Violates 2): A-B-D-C-E
    \begin{itemize}
        \item All notes are accounted for without repetition, but the original order is not maintained.
    \end{itemize}
\end{itemize}

We define $JETSING_{ROW}$ below:
\begin{itemize}
    \item $E = \sum errors $
    \item $M = \frac{T}{L}$
    \item $JETSING_{ROW} = \frac{E}{M}$
\end{itemize}

Where M represents the number of tone rows that fit within the given piece.
(T representing the length of a tone row, and L representing the length of the piece).
Calculating the $JETSING_{ROW}$ score this way gives an average over the whole piece with respect to the number of correct possible tone rows.

\subsubsection{$JETSING_{OCT}$}
This metric handles the third rule of serialism.
Since the rule simply allows a pitch to be of any octave it doesn't have a binary rule break.
In light of this we separated the rule into its own metric $JETSING_{OCT}$.

In this metric we measure the octave difference between notes $p_t$ and $p_{t+1}$, where $p$ is the pitch at time $t$.
If these two notes are in the same octave the metric reports a 0.
If the second note is at least one octave higher or lower the metric reports a 1 and so on.
This metric reports the difference in octaves from one note to the next.

We define the $JETSING_{OCT}$ below:

\begin{itemize}
    \item $\alpha_t = \Omega_{p_{t+1}} - \Omega_{p_t}$
    \item $A = \frac{\sum_{t=0}^{n-1} \alpha_t}{n-1}$
    \item $JETSING_{OCT} = \frac{A}{L-1} $
\end{itemize}

$\alpha_t$ represents the octave, $\Omega$, difference of pitches $p_{t+1}$ and $p_t$.
We then sum over $\alpha$ at each time step, to get the total octave differences in the piece,
and then divide by one less than the number of notes in the piece, $n$, to get the average octave difference over the piece, $A$.

Calculating the $JETSING_{OCT}$ this way yields the average octave difference from note to note over the whole piece.

\subsubsection{$JETSING$}
Now that we've defined the two submetrics to be used, we define the $JETSING$ metric to be:
\\ \\
\LARGE
$ JETSING = \frac{JETSING_{ROW}}{JETSING_{OCT}}$
\normalsize
\\ \\
\indent Higher $JETSING_{ROW}$ scores and lower $JETSING_{OCT}$ scores will make $JETSING$ as a whole get larger.
Lower $JETSING_{ROW}$ scores and higher $JETSING_{OCT}$ scores will make $JETSING$ as a whole get smaller.
A high $JETSING$ score indicates a good quality piece, while a lower score indicates it is more serial (and therefore less appealing).

\section{Experiments}
For this experiment we used pretrained models provided by Magenta to generate music, then rated their performances using our metric.
The models we used were Music Transformer by \cite{huang2018music}, Performance RNN by \cite{performance-rnn-2017}, and MusicVAE by \cite{musicVAE}.
We generated ten samples from each model to score with our metric.

As a baseline, we also applied $JETSING$ to music created by humans and qualitatively analyzed its measurements.
We then compared each model's average score on our metric to their scores with the metrics used by the original authors.
Finally we contrasted our baseline results to the machine-generated results.

\subsection{Datasets}
Since we used pretrained models, we had no need to find data sets for this paper.

We sourced serial music pieces from online to act as a baseline.
The pieces chosen were written by renowned serialist composers Anton Webern, Arnold Schönberg, Pierre Boulez, and  Karlheinz Stockhausen.

\section{Results}

Examining the results of the $JETSING$ metric shows that some model outputs are in fact less or more serial than others.
Our base line of serialist composers' music shows our baseline for serialism is a $JETSING$ score of about 20 or less.
Based on this value it appears that the Transformer and VAE models were able to generate four non-serial pieces,
while the RNN was only able to output a single non-serial piece of music.

\begin{table}
    \centering
    \caption{The JETSING scores for each ML model.}
    \begin{tabular}{c c c c}
        \hline
        JETSING & Transformer & VAE & RNN \\
        \hline
        MIDI 1 & 50.68 & 52.84 & 38.48 \\
        \hline
        MIDI 2 & 45.98 & 12.67 & 18.75 \\
        \hline
        MIDI 3 & 25.28 & 27.41 & 9.6 \\
        \hline
        MIDI 4 & 12.67 & 4.22 & 8.91 \\
        \hline
        MIDI 5 & 17.31 & 15.71 & 17.87 \\
        \hline
        MIDI 6 & 14.71 & 83.77 & 16.36 \\
        \hline
        MIDI 7 & 15.55 & 4.09 & 13.44 \\
        \hline
        MIDI 8 & 73.95 & 20.16 & 9.48 \\
        \hline
        MIDI 9 & 7.38 & 130.29 & 12.66 \\
        \hline
        MIDI 10 & 9.53 & 6.11 & 16.47 \\
        \hline
        \hline
        \textbf{Average} & \textbf{27.30} & \textbf{35.73} & \textbf{16.02} \\
        \hline
        \textbf{Std. Dev.} & \textbf{22.11} & \textbf{41.71} & \textbf{8.62} \\
        \hline
    \end{tabular}
\end{table}

Interestingly and unexpectedly, our metric shows that the VAE outperformed the Transformer model.
Both MIDI 6 and MIDI 9 from VAE scored higher than any of those generated by the Transformer model.
As expected, the Performance RNN model scored the lowest indicating that its pieces were more serial and thus sound less good to our ears.

We can see how consistent a model is in the quality of their output by examining the average and standard deviation of each model's $JETSING$ scores.

\begin{table}[h!]
    \centering
    \caption{The JETSING scores for each serial piece.}
    \begin{tabular}{c c}
        \hline
        JETSING & Serialist Composers \\
        \hline
        MIDI 1 & 14.03 \\
        \hline
        MIDI 2 & 10.13 \\
        \hline
        MIDI 3 & 10.62 \\
        \hline
        MIDI 4 & 12.98 \\
        \hline
        MIDI 5 & 21.91 \\
        \hline
        MIDI 6 & 16.05 \\
        \hline
        \hline
        \textbf{Average} & \textbf{14.29} \\
        \hline
        \textbf{Std. Dev.} & \textbf{4.33} \\
        \hline
    \end{tabular}
\end{table}

However, it is important to note that we were unable to gather as much data for the serialist composers as we did for the models because they weren't always available in MIDI format.
Therefore the average and standard deviation for the serialist composers may not be completely fair to judge the models against.
It is also possible that the human composers, even when trying to make serial music, still impart their inherent knowledge of good musical concepts so that the composition is remains interesting to them.

\section{Evaluation}

\subsection{$JETSING$}
Using our gathered data, the serialist composers are by far the most consistent as their standard deviation is 4.33 and we can see a score that represents a very serial piece is around a 14.

The most consistent model was the Performance RNN. While it scored the lowest (in fact its average suggests it is creating serial music), it was outputting MIDI files that were similar under our metric.

The VAE on the other hand was the least consistent, having the highest standard deviation.
While it was able to achieve the highest scoring MIDI, it also produced the lowest scoring MIDI.

In the middle, the Transformer model was twice as consistent as the VAE and produced as many non-serial pieces.
However, the Transformer's lowest scoring output is about three points better than the VAE's, but the VAE's highest score is ~60 points higher than that of the Transformer. 

While this should indicate that the VAE is producing highly non-serial music, it could be that our metric is skewed towards the octave-score.
The high-scoring VAE MIDIs tended to have many repeating notes and or many notes all in the same octave.
This phenomenon reduces the octave score to a very small number (0-1), thus dramatically increasing the $JETSING$ score.

\subsection{$JETSING$ vs NLL}

The purpose of $JETSING$ is to give more noticeable measurements of a ML model's MIDI generations to in turn provide better insight into the features of said model.
While negative log-likelihood is often the de facto standard metric for a generic model's performance,
it is not effective for distinguishing the quality and features produced by music generation models.

\begin{table}[h!]
    \centering
    \caption{The NLL scores for each ML model.}
    \begin{tabular}{c c c}
        \hline
        NLL & Transformer & Performance RNN \\
        \hline
        NLL & 1.84 & 1.969 \\
        \hline
    \end{tabular}
\end{table}

Above are the NLL scores each model scored in their original papers.
The Performance RNN and Transformer models both use NLL to score their performance on the Piano-e-Competition dataset.
The authors of the VAE model utilized other metrics, so we exclude that model for this comparison.

While this shows how well a model predicts with respect to its dataset, it doesn't quantize the actual generated outputs and as such does not provide a way to compare outputs of different models.
According to the table above, both the Transformer and Performance RNN fit relatively well to the same dataset, with the Transformer coming out slightly ahead.

However, using the $JETSING$ metric on these pre-trained models' outputs we find varying scores that highlight different features of these models' outputs.
This lends more insight into what exactly these models are generating, such as quality of output and the consistency of that quality.

\section{Conclusion}

Our experiment shows that our $JETSING$ metric is able to ascertain the tastefulness or distastefulness of a musical piece.
By comparing the $JETSING$ scores of various models, we are able to quantitatively tell which models would sound better to human ears.

Music models can be more easily assessed by using $JETSING$, a metric that is more understandable for humans, alongside a more standard ML metric.

\section{Future Work}

In the future, the metric could be improved by introducing hyperparameters to reduce the impact of repetition and the impact of same-octave clusters.
This would allow the metric to produce scores more representative of a musical piece's quality.

To get more robust statistics for these models gathering more generated MIDIs and examining their $JETSING$ scores could provide a more accurate insight.

The MIDI parser is arguably the piece in need of most refactoring.
Currently, the parser assumes that the input MIDI file consists of a right and left hand separation of music.
To improve on this, a system could be introduced to determine the number of tracks or “hands” a MIDI file has to separate.
The parser also assumes that any note higher than the middle note is played by the right hand which might not always be the case.

\section{Final Notes}

\subsection{Ethical Considerations}

It is important to note that most all of this project has oriented around Western musical theory,
so it is not necessarily representative of all music.

Outside of this consideration, there are few ethical dilemmas with this research.
We did not use any datasets, so there is no concern for where we would have sourced data.
We used pretrained models, so compute time and power is minimized.
Lastly, it is hard to think of how anyone might misuse the $JETSING$ metric for nefarious purposes. 

\subsection{Project Repository}
The code for the project as well as this paper can be found at:

\url{https://github.com/TaylorKNoah/NLP-Group-Project-Repo}

\bibliographystyle{acl_natbib}
\bibliography{works_cited}

\nocite{huang2018music, kotecha2018generating, vaswani2017attention, zhao2020verticalhorizontal, performance-rnn-2017, musicVAE}

\end{document}
