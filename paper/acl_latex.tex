% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{3cm}
%
% and set <dim> to something 5cm or larger.

\title{Measuring Music Quality Using JETSING \\ (JET Serialism Is Not Good) }

\author{Jonathan Sabini \And Ethan Fleming \And Taylor Noah 
\AND
Portland State University, 2022}

\begin{document}
\maketitle

\begin{abstract}
    The most advanced music generation models to date unsurprisingly use musical theory to work effectively.
    However, the metrics used to score these music generation models are intended for generic machine learning purposes.
    Often times this leads to many models sounding dramatically different, while having scores that are indistinguishable.
    Additionally these metrics are dependent on a model's internal weights, rather than scoring the model for its music alone.
    Shouldn't the metric scoring the models also be designed with music theory in mind?
\end{abstract}

\section{Introduction}
Music is inherently repetitive, reusing motifs to form a compelling melody that's self-coherent throughout the piece.
Many current models for music generation often have no notion of a long-term structure in musical composition and sound,
as though the computer musician is wandering around unsure how to compose a consistent rhythm.
The Music Transformer model (Huang et al., 2018) uses close attention to overcome this issue,
maintaining long-range coherence throughout its generated musical pieces.

Although the music sounds drastically better than models that don't use relative self-attention,
the negative log-likelihood metric's numerical results don't demonstrate this difference well.
Many metrics, including the negative log-likelihood, can only be utilized on a machine learning model and wouldn't be able to judge a stand-alone music piece.
In general, from what we have found, most metrics used to evaluate music generation models have little foundation on musical theory.

\section{Related Work}
Countless papers have been written on the topic of music generation, but many of them only use an understanding of musical theory to improve the models themselves.
\citep{huang2018music} worked with Vaswani to utilize the self-attention mechanisms of their original transformer model from \citep{vaswani2017attention}, but adjusting it for music generation.
\citep{kotecha2018generating} used a bi-axial LSTM to generate polyphonic music aligned with musical rules.
Finally, \citep{zhao2020verticalhorizontal} using an extensive knowledge of musical theory worked to create a lightweight variational auto-encoder model.
While many papers have used musical theory extensively to improve music generation models, we hope to create a metric that will simplify the process of assessing these models numerically.

\section{Motivation}
Our group has an interest and background in music, so we wanted to incorporate this knowledge into our machine learning research.
There are many models for music generation, but many popularly used metrics to judge them result in dramatically different qualities of musical pieces being considered relatively the same.
Many music papers in artificial intelligence, to our knowledge, are often forced to resort to qualitatively comparing generated pieces when numerical metrics fail to show the contrast.

We want a metric that will more effectively ascertain how tasteful a composition sounds or perhaps how distasteful.
Listening to the sample music generated by the relative self-attention transformer model inspired us to do something on this topic.

\subsection{Definition of Serialism}
The rules of serialism are as follows:
\begin{enumerate}
    \item No note should be repeated until all 12 notes of a note row have been played.
    \item The order of the row remains consistent throughout the piece.
    \item Notes can be played at any octave.
\end{enumerate}

\subsection{Main Goal}
We propose a metric based on the rules of serialism in western music theory, which we call the $JETSING$ metric.
In western music, which most of us are accustomed to, there is a central tonality from which notes and keys are chosen to sound “good.”
Conversely, serialism has no central tonality. Instead, it is based on the concept of a “row.”
A row is a random ordering of the 12 unique half-step pitches in western music theory (C - B on the piano).
In strict serialism, you may not repeat a pitch until all 12 pitches in a row have been played, the row must always appear in the same order, and notes are allowed to be in any octave.
Because there is no central tonality in this method, music based on serialism tends to sound creepy, confusing, lost, and seemingly aimless.

Many models that audibly perform worse tend to have characteristics of serialism.
They wander aimlessly, do not repeat motifs, and lack a sense of central tonality.
Conversely, the models that perform well tend to have central tonality and can repeat motifs and create a long-term sense of direction in the piece.

Our $JETSING$ metric is based on the rules of serialism and captures how well a piece performs with regard to those rules.
In other words, we measure how serial a piece is.
The less serial a piece is, the better it adheres to western music theory and likely sounds “good” to our ears.
$JETSING$ is comprised of two sub metrics $JETSING_{ROW}$ and $JETSING_{OCT}$.
The former metric measures how well a piece adheres to the first two rules and the latter metric measures the average octave difference between each pair of melodic notes.

\section{$JETSING$}
Based on the rules of serialism, we first create two sub-metrics of $JETSING$: $JETSING_{ROW}$ and $JETSING_{OCT}$.

\subsection{$JETSING_{ROW}$}
$JETSING_{ROW}$ will be responsible for the first two rules.
It begins at 0 and increments for each break of rules 1 and 2. 
We divide this sum by the highest possible number of correct tone rows the piece could have had.

Here are examples and valid and invalid note orderings using just 5 tones:
\begin{itemize}
    \item Valid Row: A-B-C-D-E 
    \item Invalid (Violates 1): A-A-B-C-D-E
    \begin{itemize}
        \item All notes are accounted for, but a note is repeated before all notes are played.
    \end{itemize}
    \item Invalid (Violates 2): A-B-D-C-E
    \begin{itemize}
        \item All notes are accounted for without repetition, but the original order is not maintained.
    \end{itemize}
\end{itemize}

We define JETSING-ROW below:
\begin{itemize}
    \item $E = \sum errors $
    \item $M = (T / L)$
    \item $JETSING_{ROW} = (E / M)$
\end{itemize}

Where M represents the number of tone rows that fit within the given piece.
(T representing the length of a tone row, and L representing the length of the piece).
Calculating the $JETSING_{ROW}$ score this way gives an average over the whole piece with respect to the number of correct possible tone rows.

\subsection{$JETSING_{OCT}$}
This metric handles the third rule of serialism.
Since the rule simply allows a pitch to be of any octave it doesn't have a binary rule break.
In light of this we separated the rule into its own metric $JETSING_{OCT}$.

In this metric we measure the octave difference between notes $p_t$ and $p_{t+1}$, where $p$ is the pitch at time $t$.
If these two notes are in the same octave the metric reports a 0.
If the second note is at least one octave higher or lower the metric reports a 1 and so on.
This metric reports the difference in octaves from one note to the next.

We define the $JETSING_{OCT}$ below:

\begin{itemize}
    \item $\alpha_t = \Omega_{p_{t+1}} - \Omega_{p_t}$
    \item $A = \frac{\sum_{t=0}^{n-1} \alpha_t}{n-1}$
    \item $JETSING_{OCT} = \frac{A}{L-1} $
\end{itemize}

$\alpha_t$ represents the octave, $\Omega$, difference of pitches $p_{t+1}$ and $p_t$.
We then sum over $\alpha$ at each time step, to get the total octave differences in the piece,
and then divide by one less than the number of notes in the piece, $n$, to get the average octave difference over the piece, $A$.

For each octave difference between notes $n_t$ and $n_{t+1}$ the metric reports the octave difference.
We take the sum of octaves differences and divide by number of pitches -1 in the piece.

Calculating the $JETSING_{OCT}$ this way yields the average octave difference from note to note over the whole piece.

\subsection{$JETSING$}
Now that we've defined the two submetrics to be used, we define the $JETSING$ metric to be:
\\ \\
\LARGE
$ JETSING = \frac{JETSING_{ROW}}{JETSING_{OCT}}$
\normalsize
\\ \\
\indent Higher $JETSING_{ROW}$ scores and lower $JETSING_{OCT}$ scores will make $JETSING$ as a whole get larger.
Lower $JETSING_{ROW}$ scores and higher $JETSING_{OCT}$ scores will make $JETSING$ as a whole get smaller.
A high $JETSING$ score indicates a good quality piece, while a lower score indicates it is more serial (and therefore less appealing).

\section{Experiments}
For this experiment we are using pretrained models to generate music, then rating their performances using our metric.
The models we used were Music Transformer by \cite{huang2018music}, Performance RNN by \cite{performance-rnn-2017}, and MusicVAE by \cite{musicVAE}.

As a baseline, we have applied JETSING to music created by humans and qualitatively analyzed its measurements.
We then compare each model's average score on our metric to their scores with the metrics used by the original authors.
Finally we compare our baseline results to the machine-generated results.

\subsection{Datasets}
Since we used pretrained models, we had no need to find data sets for this paper.

\section{Results}

\section{Conclusion}

\section{Future Work}

\bibliographystyle{acl_natbib}
\bibliography{works_cited}

\nocite{huang2018music, kotecha2018generating, vaswani2017attention, zhao2020verticalhorizontal, performance-rnn-2017, musicVAE}

\end{document}
